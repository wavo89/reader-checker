===== ./app.py =====
from flask import Flask, request, jsonify, render_template, send_from_directory
import openai
import os
import subprocess
import pathlib
from utils.calculate_accuracy import calculate_accuracy, preprocess_text
import json

app = Flask(__name__)
openai.api_key = os.getenv("OPENAI_API_KEY")


@app.route("/")
def index():
    scene_id = request.args.get("scene", "hex1")
    with open("scenes/scenes.json", "r") as f:
        scenes = json.load(f)
        scene = scenes.get(scene_id, scenes["hex1"])
    return render_template("index.html", scene=scene, scene_id=scene_id)


@app.route("/get-scene")
def get_scene():
    scene_id = request.args.get("scene", "hex1")
    with open("scenes/scenes.json", "r") as f:
        scenes = json.load(f)
        scene = scenes.get(scene_id, scenes["hex1"])
    return jsonify(scene)


@app.route("/scenes/<filename>")
def serve_scene_file(filename):
    return send_from_directory("scenes", filename)


@app.route("/transcribe-audio", methods=["POST"])
def transcribe_audio():
    audio_file = request.files.get("audio")

    if audio_file:
        audio_file_size = len(audio_file.read())
        audio_file.seek(0)
        size_limit = 1 * 1024 * 1024

        if audio_file_size > size_limit:
            return jsonify({"error": "Audio file size exceeds 1MB. Not processing."})

        print("Audio received. Processing...")
        original_audio_dir = pathlib.Path("audio-input/original")
        converted_audio_dir = pathlib.Path("audio-input/converted")
        original_audio_dir.mkdir(parents=True, exist_ok=True)
        converted_audio_dir.mkdir(parents=True, exist_ok=True)

        existing_files = list(original_audio_dir.glob("*.wav"))
        print(f"Existing files in original directory: {existing_files}")
        next_file_num = len(existing_files) + 1
        temp_filename = original_audio_dir / f"audio_{next_file_num}.wav"

        audio_file.save(temp_filename)
        print(f"Saved new audio file as: {temp_filename}")

        converted_filename = (
            converted_audio_dir / f"converted_audio_{next_file_num}.wav"
        )
        result = subprocess.run(
            ["ffmpeg", "-i", str(temp_filename), str(converted_filename)],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            return jsonify({"error": "Audio conversion failed."})

        if not converted_filename.exists():
            return jsonify({"error": "Converted audio file not found."})

        with open(converted_filename, "rb") as f:
            response = openai.Audio.transcribe("whisper-1", f)
            transcribed_text = response["text"]
            print("Transcription completed:", transcribed_text)

            # Preprocess the transcribed_text
            transcribed_text = preprocess_text(transcribed_text)

            # Compare the transcribed audio with each choice
            accuracies = {}
            for key in request.form.keys():
                if key.startswith("choice_"):
                    choice_text = request.form.get(key)
                    accuracy = calculate_accuracy(choice_text, transcribed_text)
                    accuracies[key] = accuracy
                    print(
                        f"Comparing with {choice_text}"
                    )  # <-- This line prints the choice text
                    print(f"Calculated Accuracy for {key}: {accuracy}")

            # Determine which choice is closer to the transcription
            closest_choice = max(accuracies, key=accuracies.get)
            closest_choice_accuracy = accuracies[closest_choice]
            print(f"Closest Choice: {closest_choice}")

            return jsonify(
                {
                    "transcript": transcribed_text,
                    "accuracies": accuracies,
                    "closest_choice_id": closest_choice,  # Return the ID of the button element
                    "closest_choice_accuracy": closest_choice_accuracy,
                }
            )

    return jsonify({"error": "No audio received."})


@app.route("/check-accuracy", methods=["POST"])
def check_accuracy_route():
    original_text = request.form.get("originalText", "")
    transcript = request.form.get("transcript", "")
    print(f"Original Text: {original_text}")
    print(f"Transcript: {transcript}")
    accuracy = calculate_accuracy(original_text, transcript)
    print(f"Calculated Accuracy: {accuracy}")
    return jsonify({"accuracy": accuracy})


if __name__ == "__main__":
    app.run(debug=True, port=5001)


===== static/js/scripts.js =====
function navigateToChoice(buttonElement) {
  const blurOverlay = document.getElementById("blurOverlay");
  if (blurOverlay) {
    blurOverlay.style.animation = "blurFadeIn 1s forwards";
  }
  const sceneLink = buttonElement.getAttribute("data-link");

  resetUIAfterTransition(); // Resetting the UI

  loadScene(sceneLink, true);
}
function resetUIAfterTransition() {
  document.getElementById("recordButton").innerText = "Record";
  document.getElementById("transcriptResult").innerText = "Recording: ";
  document.getElementById("accuracyResult").innerHTML =
    "Accuracy:&nbsp;&nbsp;----"; // Use innerHTML here
  const choiceButtons = document.querySelectorAll("#choiceButtons button");
  choiceButtons.forEach((button) => {
    button.style.border = "none";
  });
}

// ... [The other functions remain unchanged]

// ... [Other functions remain unchanged]
let nextSceneImagesPreloaded = false; // New variable to track next scenes' image preload status

function loadScene(sceneId, updateURL = false) {
  const contentWrapper = document.getElementById("contentWrapper");

  // Begin by fading out the current content
  contentWrapper.classList.add("fadeOutAnimation");
  contentWrapper.classList.remove("fadeInAnimation");
  contentWrapper.style.opacity = "0"; // Hide the content during the transition

  preloadImage(sceneId, "high")
    .then((highResUrl) => {
      // Replace the background image with the high-res image.
      document.body.style.backgroundImage = `url(${highResUrl})`;
      return fetch(`/get-scene?scene=${sceneId}`);
    })
    .then((response) => response.json())
    .then((scene) => {
      document.getElementById("sceneText").innerText = scene.text;

      const choiceButtons = document.querySelectorAll("#choiceButtons button");
      choiceButtons.forEach((button, index) => {
        if (scene.choices && scene.choices[index]) {
          button.innerText = scene.choices[index].text;
          button.setAttribute("data-link", scene.choices[index].link);
        } else {
          button.style.display = "none"; // Hide any extra buttons that are not used in this scene
        }
      });

      // Reset the UI while the container is fully invisible
      resetUIAfterTransition();

      // Fade in the updated content after a slight delay (to ensure the image transition is smooth)
      setTimeout(() => {
        contentWrapper.style.opacity = "1";
        contentWrapper.classList.remove("fadeOutAnimation");
        contentWrapper.classList.add("fadeInAnimation");
      }, 500);

      if (updateURL) {
        history.pushState(null, "", `/?scene=${sceneId}`);
      }

      // Preload images for the next possible scenes
      if (scene.choices) {
        Promise.all(
          scene.choices.map((choice) => preloadImage(choice.link, "high")),
        ).then(() => {
          nextSceneImagesPreloaded = true; // Update the status once all images are preloaded
        });
      }
    });
}
function preloadImage(sceneId, quality = "low") {
  return new Promise((resolve, reject) => {
    const imageUrl = `https://storyscenes.blob.core.windows.net/background-${
      quality === "low" ? "small" : "normal"
    }/${sceneId}.jpg`;
    const image = new Image();

    image.onload = () => {
      resolve(imageUrl);
    };

    image.onerror = reject;

    image.src = imageUrl;
  });
}

document.addEventListener("DOMContentLoaded", function () {
  // Create a new Image object to load the low-quality image.
  const lowQualityImage = new Image();
  console.log("Starting to load the low-quality image...");
  document.body.style.display = "none";

  lowQualityImage.onload = function () {
    console.log("Low-quality image has finished loading!");
    document.body.style.display = "block";
    document.getElementById("contentWrapper").style.opacity = "1"; // Show the content once low-res is loaded

    loadHighResImage();
  };

  const sceneIdElement = document.getElementById("sceneId");
  const sceneId = sceneIdElement
    ? sceneIdElement.getAttribute("data-scene-id")
    : null;
  if (sceneId) {
    const lowResImageURL =
      "https://storyscenes.blob.core.windows.net/background-small/" +
      sceneId +
      ".jpg";
    lowQualityImage.src = lowResImageURL;
  }

  function loadHighResImage() {
    const highQualityImage = new Image();
    console.log("Starting to load the high-quality image...");

    highQualityImage.onload = function () {
      console.log("High-quality image has finished loading!");
      document.body.style.backgroundImage = "url(" + highQualityImage.src + ")";

      // Here we preload images for the next possible scenes after the high-res image has loaded
      const choiceButtons = document.querySelectorAll("#choiceButtons button");
      choiceButtons.forEach((button) => {
        const nextSceneId = button.getAttribute("data-link");
        preloadImage(nextSceneId, "high");
      });
    };

    highQualityImage.onerror = function () {
      console.error("Error loading the high-quality image.");
    };

    if (sceneId) {
      highQualityImage.src =
        "https://storyscenes.blob.core.windows.net/background-normal/" +
        sceneId +
        ".jpg";
    }
  }

  document.addEventListener("keydown", function (event) {
    if (event.code === "Space") {
      const recordButton = document.getElementById("recordButton");
      if (recordButton.innerText === "Continue") {
        navigateToHighlightedChoice();
      } else {
        toggleRecording();
      }
      event.preventDefault();
    }
  });

  document
    .getElementById("recordButton")
    .addEventListener("click", function () {
      if (this.innerText === "Continue") {
        navigateToHighlightedChoice();
      } else {
        toggleRecording();
      }
    });

  let mediaRecorder;
  let audioChunks = [];
  let isRecording = false;
  let recordingTimer;

  function toggleRecording() {
    isRecording ? stopAndTranscribe() : startRecording();
  }

  function startRecording() {
    navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {
      initializeMediaRecorder(stream);
      mediaRecorder.start();
      updateUIForRecording();
      recordingTimer = setTimeout(() => {
        if (isRecording) {
          alert("Reached 25 seconds limit! Stopping recording.");
          cancelRecording();
        }
      }, 25000);
    });
  }

  function initializeMediaRecorder(stream) {
    audioChunks = [];
    mediaRecorder = new MediaRecorder(stream);
    mediaRecorder.ondataavailable = (event) => {
      audioChunks.push(event.data);
    };
  }

  function updateUIForRecording() {
    document.getElementById("recordButton").innerText = "Stop Recording";
    isRecording = true;
  }

  function stopAndTranscribe() {
    clearTimeout(recordingTimer);
    mediaRecorder.onstop = processTranscription;
    mediaRecorder.stop();
    cleanupMediaRecorder();
  }

  function cancelRecording() {
    clearTimeout(recordingTimer);
    cleanupMediaRecorder();
    document.getElementById("recordButton").innerText = "Record";
    isRecording = false;
  }

  function cleanupMediaRecorder() {
    let tracks = mediaRecorder.stream.getTracks();
    tracks.forEach((track) => track.stop());
  }

  let isTranscribing = false;

  function processTranscription() {
    document.getElementById("recordButton").innerText = "Transcribing...";
    isTranscribing = true;
    isRecording = false;

    sendAudioForTranscription().then((data) => {
      displayTranscriptionResults(data);
    });
  }

  function sendAudioForTranscription() {
    const audioBlob = new Blob(audioChunks);
    console.log("Sending audio for transcription...");

    const formData = new FormData();
    formData.append("audio", audioBlob);

    const choiceButtons = document.querySelectorAll("#choiceButtons button");
    choiceButtons.forEach((button, index) => {
      formData.append(`choice_${index + 1}`, button.innerText);
    });

    return fetch("/transcribe-audio", {
      method: "POST",
      body: formData,
    }).then((response) => response.json());
  }

  function displayTranscriptionResults(data) {
    document.getElementById(
      "transcriptResult",
    ).innerText = `Recording: ${data.transcript}`;
    isTranscribing = false;
    console.log("Transcript from Whisper:", data.transcript);

    const choiceButtons = document.querySelectorAll("#choiceButtons button");
    choiceButtons.forEach((button) => {
      button.style.border = "none";
    });

    if (data.closest_choice_accuracy !== 1) {
      const closestChoiceButton = document.getElementById(
        data.closest_choice_id,
      );
      if (closestChoiceButton) {
        closestChoiceButton.style.border = "3px solid #39FF14";
      }
    }

    let accuracyResult = document.getElementById("accuracyResult");
    accuracyResult.innerHTML = `Accuracy: ${getAccuracyEmoji(
      data.closest_choice_accuracy,
    )}`;

    const recordButton = document.getElementById("recordButton");
    if (
      data.closest_choice_accuracy === 3 ||
      data.closest_choice_accuracy === 2
    ) {
      recordButton.innerText = "Continue";
    } else {
      recordButton.innerText = "Record";
    }
  }

  function navigateToHighlightedChoice() {
    const closestChoiceButton = document.querySelector(
      "#choiceButtons button[style='border: 3px solid rgb(57, 255, 20);']",
    );
    if (closestChoiceButton) {
      const sceneLink = closestChoiceButton.getAttribute("data-link");
      loadScene(sceneLink, true);
    }
  }

  document.querySelectorAll("#choiceButtons button").forEach((button) => {
    button.addEventListener("click", function () {
      const sceneLink = button.getAttribute("data-link");
      loadScene(sceneLink, true);
      // Preload images for the next scenes
      preloadImage(sceneLink, "high");
    });
  });

  window.addEventListener("load", function () {
    setTimeout(function () {
      const blurOverlay = document.getElementById("blurOverlay");
      if (blurOverlay) {
        blurOverlay.remove();
      }
    }, 1000);
  });

  function getAccuracyEmoji(accuracy) {
    if (accuracy === 3) return "ðŸŸ¢";
    if (accuracy === 2) return "ðŸŸ¡";
    return "ðŸ”´";
  }
}); // End of DOMContentLoaded


===== templates/index.html =====
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Accuracy Checker</title>
    <link rel="stylesheet" href="static/css/styles.css">
    <style>
        body {
            background-image: url('https://storyscenes.blob.core.windows.net/background-small/{{ scene_id }}.jpg');
        }
    </style>
</head>
<body>
    <div id="blurOverlay"></div> <!-- Here's the blur overlay div -->

    <!-- Hidden div to store scene_id for JavaScript to access -->
    <div id="sceneId" data-scene-id="{{ scene_id }}" style="display: none;"></div>

    <div id="contentWrapper">
        <p id="sceneText">{{ scene.text }}</p>
        <div id="choiceButtons">
            {% for choice in scene.choices %}
            <button id="choice_{{ loop.index }}" data-link="{{ choice.link }}" onclick="navigateToChoice(this)">
                {{ choice.text }}
            </button>
                        {% endfor %}
        </div>
        <p id="transcriptResult">Recording: </p>
        <p id="accuracyResult">Accuracy:&nbsp;&nbsp;----</p>
        <button id="recordButton" onclick="toggleRecording()">Record</button>
    </div>
    <script src="static/js/scripts.js"></script>
</body>
</html>


===== utils/calculate_accuracy.py =====
import string
import re

# commit


def preprocess_text(text):
    # Remove characters not in the specified range
    text = re.sub(
        r"[^a-zA-Z\s"
        + string.punctuation
        + "Ã¡Ã©Ã­Ã³ÃºÃÃ‰ÃÃ“ÃšÃ¼ÃœÃ±Ã‘Ã§Ã‡Ã¶Ã–Ã¤Ã„Ã«Ã‹Ã¯ÃÃ¢ÃªÃ®Ã´Ã»Ã‚ÃŠÃŽÃ”Ã›Ã Ã¨Ã¬Ã²Ã¹Ã€ÃˆÃŒÃ’Ã™]",
        "",
        text,
    )
    return text


def calculate_word_accuracy(original_words, transcript_words):
    total_words = len(original_words)
    matching_weight = 0

    # Clone the lists to not modify the original lists during removals
    original_clone = original_words.copy()
    transcript_clone = transcript_words.copy()

    # Check for exact matches first
    for o in original_clone:
        if o in transcript_clone:
            matching_weight += 1
            transcript_clone.remove(o)

    # Check for partial matches among the unmatched words
    for o in [word for word in original_clone if word not in transcript_clone]:
        if len(o) > 3:  # Only consider longer words for partial matches
            for t in transcript_clone:
                common_chars = sum(1 for char in o if char in t)
                overlap_percent = common_chars / len(o)
                if overlap_percent >= 0.7:  # Threshold for partial matches
                    matching_weight += overlap_percent
                    transcript_clone.remove(t)
                    break

    # Further adjusting the penalty factor using a more aggressive approach
    missing_words = total_words - matching_weight
    base_penalty = 0.1
    dynamic_penalty = base_penalty * (10 / (total_words + 1))
    penalty_factor = 1 - (base_penalty + dynamic_penalty) * missing_words

    # Ensure word accuracy doesn't go below 0%
    return max(
        0, min(1, (matching_weight / total_words) * penalty_factor)
    )  # Ensure it doesn't exceed 100%


def longest_common_subsequence(X, Y):
    m = len(X)
    n = len(Y)
    L = [[0] * (n + 1) for i in range(m + 1)]
    for i in range(m + 1):
        for j in range(n + 1):
            if i == 0 or j == 0:
                L[i][j] = 0
            elif X[i - 1] == Y[j - 1]:
                L[i][j] = L[i - 1][j - 1] + 1
            else:
                L[i][j] = max(L[i - 1][j], L[i][j - 1])
    return L[m][n]


def calculate_order_accuracy(original_words, transcript_words):
    lcs = longest_common_subsequence(original_words, transcript_words)
    return lcs / len(original_words)


def preprocess_for_comparison(text):
    """Removes punctuation and converts text to lowercase."""
    text = text.lower()
    # Remove punctuation
    text = "".join(ch for ch in text if ch not in set(string.punctuation))
    return text


def calculate_accuracy(original_text, transcript):
    original_text = preprocess_for_comparison(original_text)
    transcript = preprocess_for_comparison(transcript)
    original_words = original_text.split()
    transcript_words = transcript.split()
    word_accuracy = calculate_word_accuracy(original_words, transcript_words)
    order_accuracy = calculate_order_accuracy(original_words, transcript_words)
    overall_accuracy = round((0.8 * word_accuracy + 0.2 * order_accuracy), 2)
    print(
        f"Word Accuracy: {word_accuracy*100:.2f}% (Weighted Matching Words: {word_accuracy*len(original_words):.2f}/{len(original_words)})"
    )
    print(
        f"Order Accuracy: {order_accuracy*100:.2f}% (Correct Order Matches: {order_accuracy*len(original_words):.2f}/{len(original_words)})"
    )
    print(f"Overall Accuracy: {overall_accuracy*100:.2f}%")

    lower_threshold = 0.40
    upper_threshold = 0.85
    if 0 <= overall_accuracy <= lower_threshold:
        return 1
    elif lower_threshold <= overall_accuracy <= upper_threshold:
        return 2
    else:
        return 3


